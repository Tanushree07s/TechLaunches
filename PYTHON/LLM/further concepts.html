<!DOCTYPE html>
<html lang="en">
  
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
      Further Concepts
    </title>
    <link rel="stylesheet" href="../../WEBSITE STYLING/style.css" />
    <link rel="stylesheet" href="../../WEBSITE STYLING/menu.css" />
    <link rel="stylesheet" href="../../WEBSITE STYLING/citations.css" />
  </head>
  
  <body>
    <main>
      <div class="main-content">
        <h2 class="collapsible ">
          nn.Module
        </h2>
        <div class="content">
          <ul>
            <span data-ref="ref27">
            </span>
            <li>
              <strong>
                <code>
                  nn.Module
                </code>
              </strong>
              is the core abstract base class for building neural networks in PyTorch,
              found in the
              <code>
                torch.nn
              </code>
              package.
            </li>
            <li>
              All custom models and layers in PyTorch are typically created by subclassing
              <code>
                nn.Module
              </code>
              .
            </li>
            <li>
              When subclassing, you define the model’s architecture inside the
              <strong>
                <code>
                  __init__
                </code>
              </strong>
              method and the computation logic in the
              <strong>
                <code>
                  forward
                </code>
              </strong>
              method.
            </li>
            <li>
              Layers such as
              <code>
                nn.Linear
              </code>
              or
              <code>
                nn.Embedding
              </code>
              declared in
              <code>
                __init__
              </code>
              are automatically registered and tracked as part of the model.
            </li>
            <li>
              This automatic tracking enables easy access to all model parameters via
              <strong>
                <code>
                  .parameters()
                </code>
              </strong>
              or
              <strong>
                <code>
                  .named_parameters()
                </code>
              </strong>
              .
            </li>
          </ul>
        </div>
        <h2 class="collapsible">
          Broadcasting
        </h2>
        <div class="content">
          <ul>
            <span data-ref="ref28">
            </span>
            <li>
              <strong>
                Broadcasting
              </strong>
              is a method that enables element-wise operations (like addition, multiplication,
              etc.) between tensors of different shapes by
              <strong>
                virtually expanding
              </strong>
              one or both tensors to make their shapes compatible.
            </li>
            <li>
              PyTorch checks shape compatibility by comparing dimensions from right
              to left (i.e., starting from the last dimension). Two dimensions are compatible
              if:
              <ol>
                <li>
                  They are equal, or
                </li>
                <li>
                  One of them is 1.
                </li>
              </ol>
            </li>
            <li>
              If the shapes are compatible, PyTorch uses
              <strong>
                virtual memory
              </strong>
              to expand the smaller tensor’s shape—without copying data—so the element-wise
              operation can proceed efficiently.
            </li>
          </ul>
          <div class="small-box">
            <ul>
              <li>For Instance:</li>
              <code>
                  <li>a = torch.tensor([[1, 2, 3], [4, 5, 6]]) # shape: (2, 3)</li>
                  <li>b = torch.tensor([10, 20, 30]) # shape: (3,)</li>
            </ul>
            </code>
            <ul>
              <li>
                  a is (2, 3)
              </li>
              <li>
                  b is (3,) → PyTorch interprets this as (1, 3).
              </li>
              <li>
                  When the 2 tensors are added PyTorch broadcasts
                  <strong>
                  b
                  </strong>
                  across the 2 rows of
                  <strong>
                  a
                  </strong>
                  .
              </li>
              <li>
                  The result is:
                  <code>
                  [[11, 22, 33],[14, 25, 36]] # shape: (2,3)
                  </code>
              </li>
            </ul>            
          </div>
        </div>
        <h2 class="collapsible">
          Dropout
        </h2>
        <div class="content">
          <ul>
            <span data-ref="ref29">
            </span>
            <li>
              <strong>
                Dropout
              </strong>
              is a regularization technique (reduces memorization) used during training
              to prevent overfitting (poor performance on new data).
            </li>
            <li>
              In each training step of a neural network, random neurons are "dropped
              out" (i.e., temporarily turned off).
            </li>
            <li>
              This means they do not participate in the forward pass or backpropagation.
            </li>
            <li>
              This prevents the model from becoming too dependent on any one neuron.
            </li>
            <li>
              Due to random selection of neurons, the network’s output may slightly
              change each time.
            </li>
            <li>
              It forces the network to learn more robust, generalized features.
            </li>
            <li>
              To ensure consistent predictions during inference, the model is set to
              evaluation mode, which disables dropout.
            </li>
          </ul>
        </div>
        <div class="nocollapse-content">
          <h3 style="padding-left:20px; font-size: larger; text-decoration: underline;">
            References:
          </h3>
          <ol id="references">
          </ol>
        </div>
      </div>
    </main>
    <script src="../../WEBSITE STYLING/script.js">
    </script>
    <script src="../../WEBSITE STYLING/menu.js">
    </script>
    <script src="../../WEBSITE STYLING/citations.js">
    </script>
  </body>

</html>